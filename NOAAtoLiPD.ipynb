{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StudyID_LiPD(NOAAStudyID):   # studyid example: 24890, 16055, 18315, # 30813: lpd file\n",
    "    metadata = queryNOAA(NOAAStudyID)\n",
    "    return getLiPD (metadata)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def queryNOAA(NOAAStudyID):\n",
    "    #    NOAA file type\n",
    "    if (NOAAStudyID >= 13000):   \n",
    "    \n",
    "        api = \"https://www.ncdc.noaa.gov/paleo-search/study/search.json?NOAAStudyId=\" + str(NOAAStudyID)\n",
    "        metadata = requests.get(api).json()\n",
    "\n",
    "        return metadata\n",
    "    \n",
    "    else:\n",
    "        print(\"Study ID below 13000 is not available.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLiPD (metadata):    \n",
    "    # convert metadata to LiPD    \n",
    "    \n",
    "    if metadata['study'][0][\"dataType\"].lower() == \"software\" or metadata['study'][0][\"dataType\"].lower() == \"repository\": \n",
    "        # e.g. studyid: 1002459, 1002682\n",
    "        print(\"Data type is \" +metadata['study'][0][\"dataType\"].lower()+\". No lpd file.\")\n",
    "    else:    \n",
    "        noaa_site = metadata['study'][0]['site']\n",
    "        lipd_pub = getPub(metadata['study'][0]['publication'])\n",
    "\n",
    "\n",
    "        Ds = list()\n",
    "\n",
    "        # different site -> different lipd (D)\n",
    "        for site in noaa_site:\n",
    "            D = dict()        \n",
    "\n",
    "            D['paleoData'] = getPaleoData (site)\n",
    "\n",
    "            D[\"createdBy\"] = \"NOAAconverter\"\n",
    "\n",
    "\n",
    "            D['geo'] = getGeo(site)\n",
    "            D['pub'] = lipd_pub\n",
    "            D['archiveType'] = getArchiveType(site)\n",
    "            D[\"dataSetName\"] = getDataSetName(D['geo']['properties']['siteName'], D['pub'],\n",
    "                                           metadata['study'][0]['investigators'])\n",
    "\n",
    "            #   originalDataUrl\": \"onlineResourceLink\"\n",
    "            D[\"originalDataUrl\"] = metadata['study'][0]['onlineResourceLink']\n",
    "\n",
    "            Ds.append(D)\n",
    "\n",
    "    #         print(D)\n",
    "\n",
    "\n",
    "        return Ds    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArchiveType(site):\n",
    "    # https://www.ncdc.noaa.gov/data-access/paleoclimatology-data/datasets\n",
    "    archiveTypeDict = {\n",
    "    \"BOREHOLE\": \"borehole\",\n",
    "    \"SPELEOTHEMS\": \"speleothem\",\n",
    "    \"CORALS AND SCLEROSPONGES\": \"coral\",    \n",
    "    \"FAUNA\": \"other\",\n",
    "    \"FIRE HISTORY\": \"other\",    \n",
    "    \"CLIMATE FORCING\": \"other\",\n",
    "    \"HISTORICAL\": \"documents\",\n",
    "    \"ICE CORES\": \"ice-other\",\n",
    "    \"PALEOLIMNOLOGY\": \"lakesediment\",\n",
    "    \"LAKE LEVELS\": \"other\",\n",
    "    \"CLIMATE RECONSTRUCTIONS\": \"other\",\n",
    "    \"LOESS AND PALEOSOL\": \"other\",\n",
    "    \"PALEOCLIMATIC MODELING\": \"other\",\n",
    "    \"PALEOCEANOGRAPHY\": \"marinesediment\",\n",
    "    \"PLANT MACROFOSSILS\": \"other\",\n",
    "    \"POLLEN\": \"lakesediment\",\n",
    "    \"INSECT\": \"other\",\n",
    "    \"TREE RING\": \"wood\",\n",
    "    \"INSTRUMENTAL\": \"other\"    \n",
    "    }   \n",
    "    \n",
    "    candidate = set()\n",
    "    for paleo in site['paleoData']:\n",
    "        for datafile in paleo['dataFile']:\n",
    "            for variable in datafile['variables']:\n",
    "                candidate = candidate.union({variable['cvDataType'].split(\"|\")[-1].upper()})\n",
    "    \n",
    "    if len(candidate) == 1:\n",
    "        try: \n",
    "            archiveType = archiveTypeDict[list(candidate)[0]]\n",
    "        except:\n",
    "            archiveType = \"other\"\n",
    "    else:\n",
    "        archiveType = \"other\"     \n",
    "        \n",
    "\n",
    "    return archiveType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSetName(sitename, pubs, investigators):\n",
    "# SiteName.Author.Year. From the NOAA API, lift as: siteName, \n",
    "# Last name of first author in investigators, most recent \"pubYear\"\n",
    "\n",
    "    year = 0\n",
    "    for pub in pubs:\n",
    "        if pub['year']:\n",
    "            if pub['year'] > year:\n",
    "                year = pub['year']\n",
    "                \n",
    "    if year == 0:\n",
    "        year = 'None'\n",
    "    else:\n",
    "        year = str(year)\n",
    "        \n",
    "    author = investigators.split(',')[0]            \n",
    "                \n",
    "    return sitename+'.'+author+'.'+year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPub(noaa_pubs):\n",
    "    lipd_pub = []\n",
    "    \n",
    "    for noaa_pub in noaa_pubs:\n",
    "        tmp_dict = dict()    \n",
    "        \n",
    "        for key, value in noaa_pub.items():\n",
    "            if key == \"pubYear\":\n",
    "                tmp_dict[\"year\"] = value\n",
    "            elif key == \"identifier\":\n",
    "                doi_flag = False\n",
    "                for k, v in noaa_pub[key].items():\n",
    "                    if k != \"type\" and k != \"id\":\n",
    "                        tmp_dict[k] = v\n",
    "                    elif k == \"type\" and v == \"doi\":\n",
    "                        doi_flag = True\n",
    "                if doi_flag:\n",
    "                    tmp_dict['doi'] = noaa_pub['identifier']['id']\n",
    "                        \n",
    "            else:\n",
    "                tmp_dict[key] = value\n",
    "        \n",
    "        lipd_pub.append(tmp_dict)\n",
    "    \n",
    "    return lipd_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGeo(noaa_geo):   \n",
    "    lipd_geo = {\"type\": noaa_geo['geo']['geoType'], \n",
    "                        \"geometry\": {\"type\": noaa_geo['geo']['geometry']['type'].capitalize(), \n",
    "                                     \"coordinates\": noaa_geo['geo']['geometry']['coordinates']\n",
    "                                    + [noaa_geo['geo']['properties']['maxElevationMeters']]\n",
    "                                    }}\n",
    "    \n",
    "    tmp = noaa_geo['geo']['properties'] \n",
    "    \n",
    "    # add other properties: e.g. 'NOAASiteId', 'siteName', 'locationName'\n",
    "    for key, value in noaa_geo.items():\n",
    "        if key != 'geo' and key != 'paleoData' and key != 'siteCode' and key !='mappable':\n",
    "            tmp[key] = value   \n",
    "    \n",
    "    lipd_geo[\"properties\"] = tmp       \n",
    "\n",
    "    return lipd_geo   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaleoData (site):    \n",
    "    paleodata_list = []\n",
    "    for paleo in site['paleoData']:\n",
    "        for noaa_paleo in paleo['dataFile']:   \n",
    "            url = noaa_paleo['fileUrl']\n",
    "            \n",
    "            if url[-3:] == \"lpd\":\n",
    "                print(url)\n",
    "                return \"tbc\"\n",
    "          \n",
    "            paleo_tables, chron_tables = getTable(url)\n",
    "            \n",
    "            for paleo_table in paleo_tables:         \n",
    "                paleodata_list.append(getPaleoDict(noaa_paleo, paleo_table))\n",
    "    \n",
    "    \"\"\"TBC: multiple tables \"\"\"\n",
    "    return paleodata_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaleoDict(noaa_paleo, table):\n",
    "    paleodata = OrderedDict()\n",
    "    paleodata['paleo0'] = OrderedDict()\n",
    "    paleodata['paleo0']['measurementTable'] = OrderedDict()\n",
    "    paleodata['paleo0']['measurementTable']['paleo0measurement0'] = dict()\n",
    "\n",
    "    paleodata['paleo0']['measurementTable']['paleo0measurement0']['tableName'] = 'paleo0measurement0'\n",
    "    paleodata['paleo0']['measurementTable']['paleo0measurement0']['missingValue'] = 'nan' \n",
    "    paleodata['paleo0']['measurementTable']['paleo0measurement0']['filename'] = None\n",
    "\n",
    "    paleodata['paleo0']['measurementTable']['paleo0measurement0']['columns'] = OrderedDict()               \n",
    "\n",
    "    variables = [] \n",
    "    cvwhats = []\n",
    "    for variable in noaa_paleo['variables']:\n",
    "        variables.append(variable['cvUnit'].split('>')[-1])        \n",
    "        cvwhats.append(variable['cvWhat'].split('>')[-1])        \n",
    "    \"\"\"need to update: matching btw columns name of table & cvWhat\"\"\"\n",
    "\n",
    "    for i, name in enumerate(table[0]):\n",
    "        paleodata['paleo0']['measurementTable']['paleo0measurement0']['columns'][name] = dict()\n",
    "\n",
    "        paleodata['paleo0']['measurementTable']['paleo0measurement0']['columns'][name]['number'] = i+1\n",
    "        paleodata['paleo0']['measurementTable']['paleo0measurement0']['columns'][name]['variableName'] = name\n",
    "\n",
    "        try:\n",
    "            paleodata['paleo0']['measurementTable']['paleo0measurement0']['columns'][name]['units'] = variables[i]\n",
    "        except:\n",
    "            paleodata['paleo0']['measurementTable']['paleo0measurement0']['columns'][name]['units'] = None\n",
    "\n",
    "\n",
    "        # value\n",
    "        value_list = []\n",
    "        for string in list(zip(*table[1:]))[i]:\n",
    "            value_list.append(string)\n",
    "        paleodata['paleo0']['measurementTable']['paleo0measurement0']['columns'][name]['value'] = value_list\n",
    "\n",
    "    return paleodata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. lift tables from a text file\n",
    "e.g. https://www.ncei.noaa.gov/pub/data/paleo/contributions_by_author/bhattacharya2018/bhattacharya2018md022517.txt\n",
    "\n",
    "# updated\n",
    "1. differentiate types of table: chronological and paleo information\n",
    "2. change missing value -> \"nan\"\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "def getTable (url):    \n",
    "    txt_data = requests.get(url).text\n",
    "    \n",
    "    # in case request error\n",
    "    while txt_data[:9] == \"<!DOCTYPE\":\n",
    "        txt_data = requests.get(url).text        \n",
    "    \n",
    "    txt_data = txt_data.replace('\\r', '')  # remove \"\\r\" in text\n",
    "    splited_data = txt_data.split('\\n')   \n",
    "    length = len(splited_data)\n",
    "    \n",
    "    \"\"\"\n",
    "    tabular data: 1. tab seperated (exception) 2. the number of columns is the same in the table.     \n",
    "    \"\"\"\n",
    "    \n",
    "    # remove the ‘#’ in front\n",
    "    # store format_index starts with \"Data line variables format\"\n",
    "    format_index = list()\n",
    "    for i in range(length):    \n",
    "        if splited_data[i].startswith(\"#\"):\n",
    "            splited_data[i] = splited_data[i][1:].lstrip()\n",
    "            \n",
    "        if splited_data[i].startswith(\"Variables\"):\n",
    "            if splited_data[i-1].startswith(\"---\"):\n",
    "                format_index.append(i+4)\n",
    "            \n",
    "    \n",
    "    # to get the number of columns\n",
    "    # count # of tab seperated words, otherwise white space seperated # of words for exception \n",
    "       \n",
    "    table_index = list()  # [[start_index, end_index],..]\n",
    "    \n",
    "    i = 0\n",
    "    while (i<length):            \n",
    "        if \"\\t\" in splited_data[i]:\n",
    "            num = len(splited_data[i].split('\\t'))\n",
    "            if num > 2:\n",
    "                start_index = i  # candidate of the first row of the table\n",
    "                end_index = i\n",
    "                i += 1            \n",
    "                while (i<length):                \n",
    "                    if len(splited_data[i].split('\\t')) == num:\n",
    "                        end_index = i\n",
    "                        i +=1\n",
    "                        \n",
    "                    else:\n",
    "                        break\n",
    "                if (end_index-start_index) > 2:\n",
    "                    table_index.append([start_index, end_index])\n",
    "            else:\n",
    "                i += 1\n",
    "                    \n",
    "        else:  # in case of white space seperated table\n",
    "            num = len(splited_data[i].split())\n",
    "            if num > 2:\n",
    "                start_index = i  # candidate of the first row of the table\n",
    "                end_index = i            \n",
    "                \n",
    "                i += 1\n",
    "                while (i<length):                \n",
    "                    if len(splited_data[i].split()) == num:\n",
    "                        end_index = i\n",
    "                        i +=1                        \n",
    "                    else:\n",
    "                        break\n",
    "                if end_index-start_index > 2:\n",
    "                    table_index.append([start_index, end_index])\n",
    "            else:\n",
    "                i += 1   \n",
    "    \n",
    "    # get tabular data  \n",
    "    \n",
    "    # differentiate types of table: chronological and paleo information      \n",
    "    chron_tables = []\n",
    "    paleo_tables = []\n",
    "    \n",
    "    for start_index, end_index in table_index:\n",
    "        \n",
    "        table = list()\n",
    "        if '\\t' in splited_data[start_index]:\n",
    "            for i in range(start_index, end_index+1):\n",
    "                table.append(splited_data[i].split('\\t'))\n",
    "                \n",
    "        else: \n",
    "            for i in range(start_index, end_index+1):\n",
    "                table.append(splited_data[i].split())\n",
    "\n",
    "                \n",
    "        # differentiate types of table\n",
    "        # btw \"----\" and the table, word \"chronology\" exists, then chronological\n",
    "        \n",
    "        missing_value = False \n",
    "        flag = True  # default: paleo info flag\n",
    "        for k in range(start_index-1, 0, -1):\n",
    "            if \"-----\" in splited_data[k]: \n",
    "                break\n",
    "\n",
    "            lower = splited_data[k].lower()\n",
    "            if \"chronology\" in lower: \n",
    "                flag = False\n",
    "\n",
    "            # missing value detect      \n",
    "            if \"missing\" in lower and \"value\" in lower:\n",
    "                missing_value = lower.split(\":\")[-1].strip()                   \n",
    "\n",
    "            # missing value => change to nan\n",
    "            if missing_value != False:\n",
    "                for row_i in range(1, len(table)):\n",
    "                    for column_i in range(len(table[0])):\n",
    "                        if table[row_i][column_i].lower() == missing_value:\n",
    "                            table[row_i][column_i] = 'nan'\n",
    "                            \n",
    "                        else:               \n",
    "                            try:\n",
    "                                missing_value2 = float(missing_value)\n",
    "                                num2 = float(table[row_i][column_i])\n",
    "\n",
    "                                if num2 == missing_value2:  \n",
    "                                    table[row_i][column_i] = 'nan'\n",
    "                            except:\n",
    "                                continue    \n",
    "                \n",
    "            \n",
    "        if flag:\n",
    "            paleo_tables.append(table)\n",
    "        else:\n",
    "            chron_tables.append(table)\n",
    "            \n",
    "#     for paleo_table in paleo_tables:        \n",
    "#         num_cols = len(paleo_table[0])\n",
    "#         for i in range(num_cols):\n",
    "#             print(splited_data[format_index[0]+i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return paleo_tables, chron_tables      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# studyid example: 24890, 16055, 18315, # 30813: lpd file\n",
    "\n",
    "Ds = StudyID_LiPD(18315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting paleoData...\n",
      "extracting: MD98-2177.Khider.2011\n",
      "Created time series: 7 entries\n"
     ]
    }
   ],
   "source": [
    "import lipd\n",
    "for D in Ds:\n",
    "    lipd.extractTs(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting paleoData...\n",
      "extracting: MD02-2515.Bhattacharya.2018\n",
      "Created time series: 7 entries\n",
      "extracting paleoData...\n",
      "extracting: MD02-2517.Bhattacharya.2018\n",
      "Created time series: 7 entries\n",
      "extracting paleoData...\n",
      "extracting: NH-8P.Bhattacharya.2018\n",
      "Created time series: 7 entries\n",
      "extracting paleoData...\n",
      "extracting: JPC-56.Bhattacharya.2018\n",
      "Created time series: 7 entries\n"
     ]
    }
   ],
   "source": [
    "Ds = StudyID_LiPD(24890)\n",
    "\n",
    "import lipd\n",
    "for D in Ds:\n",
    "    lipd.extractTs(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting paleoData...\n",
      "extracting: MD98-2181.Khider.2014\n",
      "Created time series: 7 entries\n"
     ]
    }
   ],
   "source": [
    "Ds = StudyID_LiPD(16055)\n",
    "\n",
    "import lipd\n",
    "for D in Ds:\n",
    "    lipd.extractTs(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
